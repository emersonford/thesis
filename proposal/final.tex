\documentclass[12pt,titlepage]{article}
\usepackage[letterpaper, left=1in, right=1in, top=1in, bottom=1in]{geometry}
% \documentclass[10pt,titlepage,twocolumn]{article}
% \usepackage[letterpaper, left=0.75in, right=0.75in, top=0.75in, bottom=0.75in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{amscd,amssymb,amsmath,amsthm,mathtools}
\usepackage{kpfonts}
\usepackage{titling}
\usepackage{setspace}
\usepackage{biblatex}
\usepackage{fontawesome}
\usepackage{enumitem}
\usepackage{siunitx}

\usepackage{tikz}
\usetikzlibrary{fadings}
\usetikzlibrary{patterns}
\usetikzlibrary{shadows.blur}
\usetikzlibrary{shapes}

\pagenumbering{gobble}
\addbibresource{citations.bib}
\renewcommand{\baselinestretch}{2}

\title{\Large\textbf{Evaluating the Use of RDMA in High Performance Containers} \\
Honors Computer Science Bachelor's Thesis Proposal Document}
\author{\textbf{Author}: Emerson Ford \\
\textbf{Advisor}: Ryan Stutsman \\
\textbf{Honors Faculty Advisor}: Tom Henderson \\
\textbf{Director of Undergraduate Studies}: James de St.\ Germain }
\date{\today}

\begin{document}
\maketitle

\section*{Abstract}
Containers are an increasingly popular packaging framework for complex applications, offering benefits such as lightweight isolation, portability, and ease of deployment.
These benefits have the potential to solve a myriad of issues that have long been present in the high performance computing world, presenting a compelling narrative for their use in these environments.
Unfortunately, standard container runtimes rely on relatively slow, virtualized network stacks to achieve network isolation and portability, which are incompatible with the kernel bypass networking technology RDMA\@.
This has limited the widespread adoption of containers in high performance computing environments where RDMA-reliant applications are quite common.

Fortunately, recent projects such as Microsoft's FreeFlow and Mellanox's RDMA hardware have created solutions for enabling RDMA inside of containers while maintaining varying degrees of the benefits of standard containers.
However, despite the strong claims made by these solutions, many of their characteristics such as performance, isolation sacrifices, and scalability are either not well documented or simply not known; these are characteristics that are critical to assess for several high performance computing use cases.
This thesis attempts to remedy this issue by identifying, measuring, and comparing these characteristics for the various solutions available today for enabling RDMA in containers;
ultimately providing high performance computing end-users a more holistic perspective on which solutions may be most viable for their environment and use case.

\section{Introduction}
% Why containers in HPC
% Why RDMA in HPC
Containers offer a promising solution to a host of issues that have long plagued the high performance computing (HPC) world from dependency management to portable and reproducible environments to even lightweight sharing of hardware resources;
all this is achieved through the use of statically built images and standardized container runtimes~\cite{containershpc}.
For users, cluster administrators, and application developers, this would provide more freedom and ease in application development and deployment as it eliminates the burden of working around the hundreds of possible environments on which the application may be deployed.
As an added benefit, this also opens the door for easy migration and utilization of new HPC platforms such as the cloud, Kubernetes, and SLATE that offer new compute and scheduling capabilities and treat containers as first-class objects [CITE NEEDED].

However, despite these numerous benefits, the adoption of containers in the HPC world has been notably slow [CITE NEEDED].
Among other issues like the until-recent lack of rootless container runtimes, native containers do not support Remote Direct Memory Access (RDMA), a networking technology that provides extremely low latency and high throughput with minimal CPU overhead by offloading memory accesses from the CPU to NIC hardware (hence the term ``kernel bypass networking'')~\cite{mellanoxrocerdmabenefits}.
This lack of support heavily restricts the use of native containers in HPC environments as trends like disaggregated storage, multi-machine workloads, and increased use of parallel libraries such as MPI have necessitated the use of high performance networking like RDMA to maintain performance.
To make the lack of RDMA support even worse, standard container network stacks by themselves have been shown to be both CPU intensive and significantly reduce container networking performance~\cite{abbasi2019performance}, further worsening the networking downsides of containers in HPC environments.

While it is possible to run containers without container networking by exposing the host's NIC directly to running containers (a mode called ``host networking''), this comes with several notable disadvantages:
(1) controlling fair sharing of the RDMA NIC between multiple containers cannot be done programmatically,
(2) running containers are not relocatable/portable across multiple hosts,
(3) common container orchestration tools like Kubernetes cannot be used,
and (4) network isolation and network routing policies for containers is disabled.
Thus, while this mode may be a viable for some environments, it is not adequate for the vast majority of container environments --- such as those that utilize Kubernetes heavily --- and additional solutions for enabling RDMA in containers are still needed.

Unfortunately, enabling support for RDMA in containers (and fast networking as a whole) is a nontrivial task.
Container runtimes rely on ``virtualized'' network stacks for creating flexible network overlays, enforcing isolation, and providing portability [CITE NEEDED].
These network stacks have, until recently, only been expressible through software layers provided by the host kernel, adding significant software overhead to the processing of all network packets originating from and destined to containers.
When provided by software alone, these network stacks are fundamentally incompatible with RDMA which requires bypassing the host kernel entirely for its performance guarantees.

Despite this, multiple groups have recently developed solutions for enabling RDMA in containers, each with certain sacrifices in container and/or RDMA guarantees to overcome this incompatibility.
The challenge is then to determine which solution (and subsequent sacrifices in guarantees) works best for a given environment.
This is the core problem this thesis seeks to address.
It is currently unnecessarily difficult to pick a solution due to a lack of exhaustive performance data and thorough explanations on exactly which guarantees are sacrificed for these various solutions.
% For example, it appears that no notable performance data or isolation capabilities have been published for Mellanox's Shared HCA/SRIOV solution and papers like Microsoft's FreeFlow fail to show performance data for smaller (between 2KB and 8KB) RDMA payloads.
This information is critical in HPC environments as applications may heavily rely on RDMA's low latency and/or high throughput guarantees; similarly, container security and isolation guarantees may be a requirement for those HPC environments that are multi-tenant or are running untrusted applications.

Therefore, this thesis will analyze multiple properties of the several solutions available today for enabling RDMA in containers, such as SoftRoCE~\cite{pandeysroce}, Microsoft's FreeFlow~\cite{kim2019freeflow}, MasQ~\cite{he2020masq}, and Mellanox's Shared HCA/SRIOV~\cite{mellanoxdockerroce};
these properties being grouped in two main categories:

\noindent
\textbf{Container Network Properties:}
\begin{itemize}[nolistsep]
   \item \textit{Network Isolation}: each container has its own interface, port space, etc (i.e.\ network namespace); further, a container's IP should not inherently depend on the underlying NIC's IP
   \item \textit{Controllability}: enforcing admission control, routing policies, and traffic shaping on a container with an RDMA NIC
   \item \textit{Resource Utilization}: container networks should not be CPU or memory intensive
\end{itemize}

\noindent
\textbf{RDMA Properties:}
\begin{itemize}[nolistsep]
   \item \textit{Throughput}: how closely can a solution match host RDMA throughput
   \item \textit{Latency}: what latency overhead is incurred per message for a given solution
\end{itemize}

% fabric scalability
\noindent
and analyzed with varying levels of:
\begin{itemize}[nolistsep]
   \item number of containers present across the cluster and on a single host
   \item message size of network payloads
   \item messages sent per second
\end{itemize}
which should adequately test the scalability of these properties.
Finally, comments will also be made on:
\begin{itemize}[nolistsep]
   \item \textit{Proprietary}: can a solution be used across various RDMA NIC vendors
   \item \textit{Maturity}: what support exists for a solution, how well tested is a solution
   \item \textit{Ease in Deployment}: are application changes necessary for a solution, how difficult is it to deploy the solution
   \item \textit{Execution Privileges}: can the solution be used without elevated capabilities such as \texttt{CAP\_SYS\_ADMIN}
   \item \textit{Network Pressure}: what additional pressure do these solutions put on the network
\end{itemize}
to assist in determining if a given environment can even support a given solution.

Additionally, these solutions can be generalized to three implementations: pure software, paravirtualization-like, and pure hardware;
with each solution in an implementation likely having similar properties.
Thus, this thesis should also provide general insight as to which implementations may be best suited for a given HPC environment.

% TODO: Add summary of conclusion here.

\section{Background}
\subsection{Networking Planes}
Networking can be broken down
% The field of networking is rife with jargon and overloaded terms.
% For both clarity on the terms to be used as well as to facilitate better understanding of RDMA and container networking fundamentals, the concept of the control plane and data plane of networking will be explained.

\noindent
\textbf{Control Plane} handles connection state (e.g.\ initiating, maintaining, and terminating connections), routing policies, admission control, etc.
Systems such as firewalls, routing tables, \texttt{iptables} / \texttt{nftables}, interface management, and IP assignment (e.g. DHCP) are all considered as part of the control plane.

\noindent
\textbf{Data Plane} (also known as the forwarding plane) handles the actual transmission of network data.
Systems such as memory copies into network buffers, packet transmission, and traffic shaping are considered as part of the data plane.

\subsection{RDMA Networking}
Remote Direct Memory Access (RDMA) is a network protocol designed for extremely fast and high throughput access to memory regions on remote hosts with minimal CPU overhead, usually run on top of the Infiniband network fabric or Ethernet (via RoCE).
RDMA can provide a substantial performance boost for inter-machine memory sharing and synchronization, as well as savings on CPU cycles that would otherwise be used for network operations, both .
As a result, support for RDMA can now be found in the vast majority of parallel applications and libraries, such as TensorFlow and OpenMPI, and has become ubiquitously used in the high performance computing world~\cite{kim2019freeflow}~[ADDITIONAL CITE].

At its core, RDMA is a form of kernel-bypass networking,
% a networking model that shifts the burden of the data plane and/or the control plane from the kernel to either the network card (NIC) or user-space network stacks, often with aid from technologies such as IOMMU.
% This model eliminates key networking bottlenecks, such as context switch overheads and possible user to kernel space memory copies, thereby providing potential orders of magnitude improvements in network latency and bandwidth.
meaning it shifts all data plane operations from the kernel to the network card (also called the HCA in RDMA terminology).
This eliminating bottlenecks that arise from context switches and potential kernel memory copies.


Control plane operations are also moved into interactions with the \textttt{sysfs} / \texttt{udev} systems on Linux.

current top-of-the-line RDMA enabled NICs are capable of achieving sub-1\si{\micro\second} latency with >100Gb/s flows~\cite{mellanoxcx6doc}~[ADDITIONAL CITE].
Due these impressive latency and throughput characteristics, it has become widely adopted across the high performance computing world~\cite{kim2019freeflow} [ADDITIONAL CITE].

% In contrast, traditional Linux networking relies on the kernel to manage the control plane and data plane, with user-space applications initiating operations using \texttt{syscall}s.
% In contrast, even with aggressive tuning, the Linux network stack appears to be capable of achieving only a minimum latency of 30\si{\micro\second} and a maximum of 100Gb/s flows with currently top-of-the-line Ethernet NICs~\cite{cloudflarelowlatency}~\cite{intel800nic}.

% RDMA achieve kernel-bypass by first, having user-space applications initiate control-plane operations via character devices, and second, initiating data plane operations through memory sharing.

\subsection{Containers and Container Networking}
% Containers and when they're used
% Why is virtualized networking slow?
% What is RDMA networking.
% HPC applications tend to make heavy use of specialized, highly performant network fabrics like Infiniband to shuffle data back and forth between different hosts.
% Further, these applications also often utilize RDMA to bypass the kernel's networking stack.
% Unfortunately, many container runtimes rely on the kernel's networking stack to create a \textit{virtualized} networking stack for enforcing network isolation.

% To decrease the bottleneck that is multi-machine communication, these workloads tend to heavily utilize a networking technology called Remote Direct Memory Access (RDMA); a form of kernel bypass networking that allows for access to a remote host's memory with extremely low latency and high throughput.
% However, container runtimes rely on the kernel's network stack to create a ``virtualized'' overlay network for enforcing network isolation, thus any kernel bypass networking solutions would seem to be incompatible with existing container network stacks~\cite{abbasi2019performance}.
% While containers can be run without network isolation in a mode called ``host networking'' that allow for the direct use of the RDMA network interface inside of containers, this reduces portability and disables network isolation, eliminating many of the benefits of containers~\cite{dockerhostnetworking}.

% These solutions can generally categorized as either software based/paravirtualized or hardware-based.
% Software based/paravirtualized solutions generally either entirely emulate an RDMA NIC or use software for just the control plane of the NIC, however they tend to be more resource intensive and struggle to provide the full performance capability of the underlying hardware.
% Hardware based solutions rely on features built into the NIC hardware itself, however they tend not to be generalizable to commodity hardware, are less flexible, and suffer scaling issues due to limited hardware resources.

% The reason for this lack of support is two-fold.

\subsection{Related Work}
% However, containers are quite unique in their needs and various usage patterns, which differ significantly from those of virtual machines.
% In particular, containers are unique in that they are (a) treated as more ephemeral constructs than VMs, (b) can be dynamically scaled up or down on one or many hosts, (c) a single host could run tens to hundreds of containers, and (d) heavily utilize software defined networking to facilitate inter-container communication.

% In particular, containers are unique in several key ways:
% \begin{itemize}
%    \item Containers are treated as ephemeral
%    \item A single machine could run tens to potentially hundreds of containers
%    \item Container networking is nearly all software-defined
% \end{itemize}

\subsubsection{RDMA Namespaces and Cgroups}
With namespaces and cgroups being the standard systems for container isolation in the Linux kernel, there have been developments to add RDMA implementations of these to the Linux kernel itself~\cite{rdmanamespace}~\cite{rdmacgroups}~\cite{mellanoxcontainersupdate2018}.

\subsubsection{Sharing RDMA NICs between Virtual Machines}
\subsubsection{Programmable NICs}
\subsubsection{eBPF Container Networking}
\subsubsection{DPDK in Containers}

\section{Overview}

\section{Discussion}
\subsection{Security}
% Looking forward

\section{Conclusion}


\nocite{*}
\pagebreak
\section*{References}
\printbibliography[heading=subbibliography,keyword={containers},title={Containers}]{}
\printbibliography[heading=subbibliography,keyword={containernetworking},title={Container Networking}]{}
\printbibliography[heading=subbibliography,keyword={rdmacontainers},title={RDMA in Containers}]{}
\printbibliography[heading=subbibliography,keyword={rdmaresearch},title={RDMA Specific Research}]{}
\printbibliography[heading=subbibliography,keyword={highperfnetworking},title={High Performance Networking}]{}
\printbibliography[heading=subbibliography,keyword={kernelbypassnetworking},title={Kernel Bypass Networking}]{}
\printbibliography[heading=subbibliography,keyword={documentation},title={Documentation}]{}
\printbibliography[heading=subbibliography,keyword={tutorial},title={Tutorials}]{}
\printbibliography[heading=subbibliography,keyword={mellanox},title={Mellanox Infosheets}]{}

\pagebreak
\section*{Appendix}
\appendix
\section{RDMA Two-Sided Ops Diagram}
\renewcommand{\baselinestretch}{1.15}
\resizebox{\textwidth}{!}{
   \begin{tikzpicture}[x=0.75pt,y=0.75pt,yscale=-1,xscale=1]
      \input{rdmatwosided.tikz}
   \end{tikzpicture}
}
\end{document}
